# src/training.py

import os
import gc
import math
import torch
import pandas as pd
from termcolor import cprint
from tqdm import tqdm
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling,
    TrainerCallback
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from sklearn.metrics import accuracy_score, classification_report

# ä»é¡¹ç›®å…¶ä»–æ¨¡å—å¯¼å…¥
from prompts import PROMPT_DICT
from utils import LABEL_MAP, clean_prediction, load_xml_data
from config import Config

# --- æ•°æ®å‡†å¤‡ ---

def prepare_train_dataset(df, tokenizer, prompt_func):
    """ä»…å‡†å¤‡å’Œå¤„ç†è®­ç»ƒæ•°æ®é›†ã€‚"""
    cprint("--> å‡†å¤‡è®­ç»ƒæ•°æ®é›†...", "blue")
    df['corrected_status'] = df['status'].str.strip().str.upper().apply(lambda x: LABEL_MAP.get(x, "UNKNOWN"))
    df['prompt'] = df['text'].apply(prompt_func)
    df['full_text'] = df['prompt'] + "\n" + df['corrected_status']
    train_dataset = Dataset.from_pandas(df)
    def tokenize_train(examples):
        return tokenizer(examples["full_text"], truncation=True, padding="max_length", max_length=512)
    tokenized_train_dataset = train_dataset.map(tokenize_train, batched=True, remove_columns=train_dataset.column_names)
    cprint(f"è®­ç»ƒé›†å‡†å¤‡å®Œæˆ: {len(tokenized_train_dataset)} æ¡è®°å½•", "green")
    return tokenized_train_dataset

def prepare_eval_dataset(df, prompt_func):
    """ä»…å‡†å¤‡å’Œå¤„ç†è¯„ä¼°ï¼ˆæµ‹è¯•ï¼‰æ•°æ®é›†ã€‚"""
    cprint("--> å‡†å¤‡è¯„ä¼°(æµ‹è¯•)æ•°æ®é›†...", "blue")
    df['corrected_status'] = df['status'].str.strip().str.upper().apply(lambda x: LABEL_MAP.get(x, "UNKNOWN"))
    df['prompt'] = df['text'].apply(prompt_func)
    eval_dataset = Dataset.from_pandas(df)
    cprint(f"è¯„ä¼°(æµ‹è¯•)é›†å‡†å¤‡å®Œæˆ: {len(eval_dataset)} æ¡è®°å½•", "green")
    return eval_dataset


# --- è‡ªå®šä¹‰å›è°ƒ ---

class EvaluateAndLogCallback(TrainerCallback):
    """åœ¨è®­ç»ƒå¼€å§‹å‰å’Œæ¯ä¸ªepochç»“æŸæ—¶è¯„ä¼°å¹¶è®°å½•æŒ‡æ ‡çš„è‡ªå®šä¹‰å›è°ƒ"""
    def __init__(self, eval_dataset, tokenizer):
        self.eval_dataset = eval_dataset
        self.tokenizer = tokenizer
        self.history = []

    def _do_evaluate(self, model, epoch, state=None):
        """æ‰§è¡Œè¯„ä¼°çš„æ ¸å¿ƒé€»è¾‘"""
        cprint(f"\n--- å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° Epoch {epoch}... ---", "yellow", attrs=['bold'])
        predictions, true_labels = [], []
        model.eval()
        with torch.no_grad():
            for example in tqdm(self.eval_dataset, desc=f"Evaluating Epoch {epoch} on Test Set"):
                prompt = example['prompt']
                true_label = example['corrected_status']
                inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512).to(model.device)
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=10,
                    eos_token_id=self.tokenizer.eos_token_id,
                    pad_token_id=self.tokenizer.pad_token_id,
                    temperature=0.2,
                    top_p=0.8,
                    do_sample=True
                )
                output_ids = outputs[0][inputs.input_ids.shape[1]:]
                generated_text = self.tokenizer.decode(output_ids, skip_special_tokens=True)
                predicted_label = clean_prediction(generated_text)
                predictions.append(predicted_label)
                true_labels.append(true_label)
        
        accuracy = accuracy_score(true_labels, predictions)
        unique_labels = sorted(list(set(true_labels)))
        report = classification_report(
            true_labels, 
            predictions, 
            output_dict=True, 
            labels=unique_labels, 
            zero_division=0
        )
        weighted_avg = report.get('weighted avg', {})
        f1 = weighted_avg.get('f1-score', 0.0)
        recall = weighted_avg.get('recall', 0.0)

        train_loss, eval_loss = None, None
        if state and state.log_history:
            train_loss = next((log['loss'] for log in reversed(state.log_history) if 'loss' in log), None)
            eval_loss = next((log['eval_loss'] for log in reversed(state.log_history) if 'eval_loss' in log), None)
        
        cprint(f"Epoch {epoch} è¯„ä¼°ç»“æœ (æµ‹è¯•é›†):", "green", attrs=['bold'])
        cprint(f"  - æµ‹è¯•é›†å‡†ç¡®ç‡ (Test Accuracy): {accuracy:.4f}", "green")
        cprint(f"  - æµ‹è¯•é›† F1 Score (Weighted): {f1:.4f}", "green")
        cprint(f"  - æµ‹è¯•é›†å¬å›ç‡ (Weighted Recall): {recall:.4f}", "green")
        if eval_loss is not None:
            cprint(f"  - æµ‹è¯•é›†æŸå¤± (Test Loss): {eval_loss:.4f}", "green")

        self.history.append({
            "epoch": epoch,
            "train_loss": train_loss,
            "eval_loss": eval_loss,
            "eval_accuracy": accuracy,
            "eval_f1_score": f1,
            "eval_recall": recall
        })
        model.train()

    def on_train_begin(self, args, state, control, model, **kwargs):
        cprint("--- åœ¨æ­£å¼è®­ç»ƒå¼€å§‹å‰ï¼Œæ‰§è¡ŒåŸºçº¿æ€§èƒ½è¯„ä¼° (Epoch 0 on Test Set) ---", "magenta", attrs=['bold'])
        self._do_evaluate(model, epoch=0)

    def on_epoch_end(self, args, state, control, model, **kwargs):
        self._do_evaluate(model, epoch=round(state.epoch), state=state)

# --- ä¸»è®­ç»ƒå‡½æ•° ---

def run_experiment(model_id, prompt_type, config):
    """æ‰§è¡Œå•æ¬¡å®Œæ•´çš„è®­ç»ƒå®éªŒ"""

    # --- æ­¥éª¤ 1: åŠ è½½æ•°æ® ---
    cprint("--- æ­¥éª¤ 1: åŠ è½½æ•°æ® ---", "cyan", attrs=['bold'])
    train_df = load_xml_data(config.TRAIN_DATASET_PATH)
    test_df = load_xml_data(config.TEST_DATASET_PATH)
    if train_df.empty or test_df.empty:
        cprint("è®­ç»ƒæ•°æ®æˆ–æµ‹è¯•æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æ­¤å®éªŒã€‚", "red")
        return
    train_df.dropna(subset=['text', 'status'], inplace=True)
    test_df.dropna(subset=['text', 'status'], inplace=True)
    
    # --- æ­¥éª¤ 2: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    cprint(f"--- æ­¥éª¤ 2: åŠ è½½æ¨¡å‹ '{model_id}' ---", "cyan", attrs=['bold'])
    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16)
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        quantization_config=bnb_config, 
        device_map="auto", 
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    prompt_function = PROMPT_DICT[prompt_type]
    train_dataset = prepare_train_dataset(train_df, tokenizer, prompt_function)
    test_dataset = prepare_eval_dataset(test_df, prompt_function)
    
    # --- æ­¥éª¤ 3: é…ç½® LoRA ---
    cprint("--- æ­¥éª¤ 3: é…ç½® LoRA ---", "cyan", attrs=['bold'])
    model = prepare_model_for_kbit_training(model)
    lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, bias="none", task_type="CAUSAL_LM", target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'])
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # --- æ­¥éª¤ 4: è®¾ç½®å¹¶å¼€å§‹è®­ç»ƒ ---
    cprint("--- æ­¥éª¤ 4: è®¾ç½®å¹¶å¼€å§‹è®­ç»ƒ ---", "cyan", attrs=['bold'])
    evaluation_callback = EvaluateAndLogCallback(eval_dataset=test_dataset, tokenizer=tokenizer)
    model_name_safe = model_id.replace("/", "_")
    output_dir_experiment = os.path.join(config.OUTPUT_DIR_BASE, f"{model_name_safe}_{prompt_type}")

    num_training_samples = len(train_dataset)
    # gradient_accumulation_steps = 4
    effective_batch_size = config.BATCH_SIZE * 4 
    steps_per_epoch = math.ceil(num_training_samples / effective_batch_size)
    cprint(f"è®¡ç®—å¾—å‡ºæ¯ä¸ª Epoch çš„æ­¥æ•°: {steps_per_epoch}", "blue")

    training_args = TrainingArguments(
        output_dir=output_dir_experiment,
        per_device_train_batch_size=config.BATCH_SIZE,
        gradient_accumulation_steps=4,
        num_train_epochs=config.EPOCHS,
        learning_rate=config.LEARNING_RATE,
        optim="paged_adamw_8bit",
        logging_steps=steps_per_epoch,
        # eval_strategy and save_strategy are set to "epoch" by default with save_strategy="epoch"
        save_strategy="epoch",
        fp16=True,
        report_to="none",
    )

    trainer = Trainer(
        model=model,
        train_dataset=train_dataset,
        args=training_args,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
        callbacks=[evaluation_callback],
    )
    
    cprint(f"æ¨¡å‹è®¾å¤‡åˆ†å¸ƒæƒ…å†µ: {model.hf_device_map}", "yellow")
    cprint(f"Trainerå°†ä½¿ç”¨çš„è®¾å¤‡: {trainer.args.device}", "yellow")
    
    cprint("å³å°†å¼€å§‹è®­ç»ƒ...", "green", attrs=['bold'])
    trainer.train()
    cprint("è®­ç»ƒå®Œæˆï¼", "green", attrs=['bold'])
    
    # --- æ­¥éª¤ 5: ä¿å­˜æœ€ç»ˆç»“æœ ---
    cprint(f"--- æ­¥éª¤ 5: ä¿å­˜æœ€ç»ˆç»“æœ ---", "cyan", attrs=['bold'])
    final_output_dir = os.path.join(output_dir_experiment, "final_model")
    model.save_pretrained(final_output_dir)
    tokenizer.save_pretrained(final_output_dir)
    cprint(f"æ¨¡å‹å’Œåˆ†è¯å™¨å·²æˆåŠŸä¿å­˜åˆ° {final_output_dir}ã€‚", "green")

    history_df = pd.DataFrame(evaluation_callback.history)
    history_path = os.path.join(output_dir_experiment, "training_history.csv")
    history_df.to_csv(history_path, index=False)
    cprint(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ° {history_path}ã€‚", "green")
    cprint("è®­ç»ƒå†å²è®°å½• (åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°):", "yellow")
    print(history_df)


def start_training():
    """
    å¯åŠ¨æ‰€æœ‰åœ¨Configä¸­å®šä¹‰çš„è®­ç»ƒå®éªŒã€‚
    """
    config = Config()
    
    models_to_run = config.LOCAL_MODEL_PATH if config.LOCAL_MODEL_PATH else config.MODELS_TO_RUN
    
    for model_id in models_to_run:
        for prompt_type in config.PROMPTS_TO_RUN:
            model_name_safe = model_id.replace("/", "_")
            final_model_path = os.path.join(config.OUTPUT_DIR_BASE, f"{model_name_safe}_{prompt_type}", "final_model")

            if os.path.exists(final_model_path):
                cprint(f"\n{'='*80}", "yellow", attrs=['bold'])
                cprint(f"â­ï¸  æ£€æµ‹åˆ°å·²å­˜åœ¨çš„æ¨¡å‹ç›®å½•: {final_model_path}", "yellow")
                cprint(f"è·³è¿‡è®­ç»ƒ: æ¨¡å‹ = {model_id}, Promptç±»å‹ = {prompt_type}", "yellow")
                cprint(f"{'='*80}\n", "yellow", attrs=['bold'])
                continue

            cprint(f"\n{'='*80}", "blue", attrs=['bold'])
            cprint(f"ğŸš€ å¼€å§‹æ–°å®éªŒ: æ¨¡å‹ = {model_id}, Promptç±»å‹ = {prompt_type}", "blue", attrs=['bold'])
            cprint(f"{'='*80}", "blue", attrs=['bold'])
            
            run_experiment(model_id, prompt_type, config)
            
            cprint(f"\n{'~'*80}", "magenta")
            cprint(f"âœ… å®éªŒç»“æŸ: æ¨¡å‹ = {model_id}, Promptç±»å‹ = {prompt_type}", "magenta", attrs=['bold'])
            cprint("æ¸…ç†å†…å­˜...", "magenta")
            gc.collect()
            torch.cuda.empty_cache()
            cprint(f"{'~'*80}\n", "magenta")

    cprint("ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰è®­ç»ƒå®éªŒå‡å·²å®Œæˆï¼ ğŸ‰ğŸ‰ğŸ‰", "green", attrs=['bold'])
    