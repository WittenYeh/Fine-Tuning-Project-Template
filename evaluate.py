# src/evaluate.py

import os
import glob
import gc
import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from termcolor import cprint
from tqdm import tqdm
from sklearn.metrics import cohen_kappa_score, classification_report
from transformers import AutoModelForCausalLM, AutoTokenizer

# ä»é¡¹ç›®å…¶ä»–æ¨¡å—å¯¼å…¥
from config import Config
from utils import load_xml_data, clean_prediction, LABEL_MAP
from prompts import PROMPT_DICT


# --- ç»“æœæ±‡æ€»ä¸å¯è§†åŒ– ---
def aggregate_results(base_dir, prompts_list):
    """éå†æ‰€æœ‰å®éªŒç›®å½•ï¼Œæ±‡æ€» training_history.csv æ–‡ä»¶ã€‚"""
    all_history_files = glob.glob(os.path.join(base_dir, "*", "training_history.csv"))
    
    if not all_history_files:
        cprint(f"é”™è¯¯ï¼šåœ¨ '{base_dir}' ç›®å½•ä¸‹æœªæ‰¾åˆ°ä»»ä½• 'training_history.csv' æ–‡ä»¶ã€‚", "red")
        cprint("è¯·ç¡®è®¤å®éªŒæ˜¯å¦å·²æˆåŠŸè¿è¡Œå¹¶ç”Ÿæˆäº†ç»“æœæ–‡ä»¶ã€‚", "yellow")
        return pd.DataFrame()
    
    all_dfs = []
    for f_path in all_history_files:
        try:
            experiment_name = os.path.basename(os.path.dirname(f_path))
            prompt_type = None
            model_name_safe = None
            
            # ä»åå‘å‰åŒ¹é…ï¼Œç¡®ä¿èƒ½æ­£ç¡®å¤„ç†åŒ…å«promptåçš„æ¨¡å‹å
            for p in sorted(prompts_list, key=len, reverse=True):
                if experiment_name.endswith(f"_{p}"):
                    prompt_type = p
                    model_name_safe = experiment_name[:-len(f"_{p}")]
                    break
            
            if prompt_type is None:
                cprint(f"è­¦å‘Šï¼šæ— æ³•ä»ç›®å½• '{experiment_name}' ä¸­è§£æå‡ºPromptç±»å‹ã€‚è·³è¿‡æ­¤æ–‡ä»¶ã€‚", "yellow")
                continue
            
            # æ›¿æ¢å›åŸå§‹çš„'/'ä»¥åŒ¹é…æ˜¾ç¤ºåç§°
            readable_model_name = model_name_safe.replace("_", "/")

            df = pd.read_csv(f_path)
            df['model_id'] = readable_model_name
            df['prompt_type'] = prompt_type
            all_dfs.append(df)
        except Exception as e:
            cprint(f"å¤„ç†æ–‡ä»¶ {f_path} æ—¶å‡ºé”™: {e}", "red")

    if not all_dfs:
        cprint("æœªèƒ½æˆåŠŸåŠ è½½ä»»ä½•å®éªŒæ•°æ®ã€‚", "red")
        return pd.DataFrame()
        
    return pd.concat(all_dfs, ignore_index=True)


def plot_metric_curves(results_df, base_dir):
    """ç»˜åˆ¶æ‰€æœ‰å…³é”®æŒ‡æ ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–æ›²çº¿ã€‚"""
    cprint("--> æ­£åœ¨ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡å˜åŒ–æ›²çº¿å›¾...", "blue")
    
    def plot_single_metric(metric_name, title, y_label):
        if metric_name not in results_df.columns:
            cprint(f"æŒ‡æ ‡ '{metric_name}' æœªåœ¨ç»“æœä¸­æ‰¾åˆ°ï¼Œè·³è¿‡ç»˜åˆ¶ '{title}' å›¾è¡¨ã€‚", "yellow")
            return
        
        sns.set_theme(style="whitegrid", palette="viridis")
        g = sns.FacetGrid(results_df, col="model_id", col_wrap=2, height=5, aspect=1.5, hue="prompt_type")
        g.map(sns.lineplot, "epoch", metric_name, marker='o', sort=False)
        g.add_legend(title='Prompt ç±»å‹')
        g.fig.suptitle(title, fontsize=18, y=1.03)
        g.set_axis_labels("è®­ç»ƒè½®æ¬¡ (Epoch)", y_label)
        g.set_titles("æ¨¡å‹: {col_name}")
        plt.tight_layout(rect=[0, 0, 1, 0.97])
        
        # ä¿å­˜å›¾è¡¨
        filename = f"training_curve_{metric_name}.png"
        filepath = os.path.join(base_dir, filename)
        plt.savefig(filepath, dpi=300)
        cprint(f"å·²ä¿å­˜å›¾è¡¨åˆ°: {filepath}", "green")
        plt.show()

    plot_single_metric('eval_accuracy', 'å„æ¨¡å‹åœ¨ä¸åŒPromptä¸‹çš„æµ‹è¯•é›†å‡†ç¡®ç‡å˜åŒ–æ›²çº¿', 'æµ‹è¯•é›†å‡†ç¡®ç‡ (Test Accuracy)')
    plot_single_metric('eval_f1_score', 'å„æ¨¡å‹åœ¨ä¸åŒPromptä¸‹çš„æµ‹è¯•é›† F1 Score å˜åŒ–æ›²çº¿', 'æµ‹è¯•é›† F1 Score (Weighted)')
    plot_single_metric('eval_recall', 'å„æ¨¡å‹åœ¨ä¸åŒPromptä¸‹çš„æµ‹è¯•é›†å¬å›ç‡å˜åŒ–æ›²çº¿', 'æµ‹è¯•é›†å¬å›ç‡ (Weighted Recall)')


def plot_final_performance(results_df, base_dir):
    """ä½¿ç”¨ catplot åŒæ—¶å±•ç¤º Accuracy, F1 Score, å’Œ Recall çš„æœ€ç»ˆæ€§èƒ½å¯¹æ¯”ã€‚"""
    cprint("--> æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾...", "blue")
    
    final_performance_df = results_df.loc[results_df.groupby(['model_id', 'prompt_type'])['epoch'].idxmax()].copy()
    final_metrics_melted_df = final_performance_df.melt(
        id_vars=['model_id', 'prompt_type'], 
        value_vars=['eval_accuracy', 'eval_f1_score', 'eval_recall'],
        var_name='metric', value_name='score'
    )
    
    metric_labels = {
        'eval_accuracy': 'Accuracy', 'eval_f1_score': 'F1 Score (Weighted)', 'eval_recall': 'Recall (Weighted)'
    }
    final_metrics_melted_df['metric'] = final_metrics_melted_df['metric'].map(metric_labels)

    g = sns.catplot(
        data=final_metrics_melted_df, kind='bar', x='model_id', y='score', 
        hue='prompt_type', col='metric', palette='plasma', height=6, aspect=1.2
    )
    
    g.fig.suptitle('ä¸åŒæ¨¡å‹ä¸Promptç»„åˆçš„æœ€ç»ˆæµ‹è¯•é›†æ€§èƒ½å¯¹æ¯”', fontsize=20, y=1.03)
    g.set_axis_labels("æ¨¡å‹", "åˆ†æ•°")
    g.set_titles("æŒ‡æ ‡: {col_name}")
    g.set_xticklabels(rotation=15, ha='right')
    g.set(ylim=(0, max(1.0, final_metrics_melted_df['score'].max() * 1.15)))
    g.legend.set_title("Prompt ç±»å‹")

    for ax in g.axes.flat:
        for p in ax.patches:
            ax.annotate(f'{p.get_height():.3f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha='center', va='center', xytext=(0, 9), textcoords='offset points', fontsize=9)

    plt.tight_layout(rect=[0, 0, 1, 0.97])
    
    filepath = os.path.join(base_dir, "final_performance_comparison.png")
    plt.savefig(filepath, dpi=300)
    cprint(f"å·²ä¿å­˜å›¾è¡¨åˆ°: {filepath}", "green")
    plt.show()

# --- ç”Ÿæˆé¢„æµ‹æ–‡ä»¶ ---
def generate_predictions_for_all(config):
    """åŠ è½½æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œåœ¨åŒä¸€ä¸ªå®Œæ•´çš„æµ‹è¯•é›†ä¸Šç”Ÿæˆé¢„æµ‹å¹¶ä¿å­˜ã€‚"""
    cprint("--- å‡†å¤‡å›ºå®šçš„æµ‹è¯•é›† ---", "cyan")
    test_df = load_xml_data(config.TEST_DATASET_PATH)
    if test_df.empty:
        cprint("æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆé¢„æµ‹ã€‚", "red")
        return pd.DataFrame()
        
    test_df.dropna(subset=['text', 'status'], inplace=True)
    test_df['corrected_status'] = test_df['status'].str.strip().str.upper().apply(lambda x: LABEL_MAP.get(x, "UNKNOWN"))
    cprint(f"å·²åŠ è½½å®Œæ•´çš„æµ‹è¯•é›†ï¼ŒåŒ…å« {len(test_df)} æ¡æ ·æœ¬ã€‚", "green")

    all_predictions = []
    experiment_dirs = [d for d in glob.glob(os.path.join(config.OUTPUT_DIR_BASE, "*")) if os.path.isdir(d)]
    
    if not experiment_dirs:
        cprint("æœªæ‰¾åˆ°ä»»ä½•å®éªŒç»“æœç›®å½•ï¼Œè·³è¿‡é¢„æµ‹ç”Ÿæˆã€‚", "red")
        return pd.DataFrame()

    for exp_dir in tqdm(experiment_dirs, desc="å¤„ç†æ‰€æœ‰å®éªŒ"):
        model_path = os.path.join(exp_dir, "final_model")
        if not os.path.exists(model_path):
            continue
            
        exp_name = os.path.basename(exp_dir)
        prompt_type = next((p for p in sorted(config.PROMPTS_TO_RUN, key=len, reverse=True) if exp_name.endswith(f"_{p}")), None)
        if not prompt_type: continue
        
        model_name_safe = exp_name[:-len(f"_{prompt_type}")]
        readable_model_name = model_name_safe.replace("_", "/")
        
        cprint(f"\n--- æ­£åœ¨ä¸ºæ¨¡å‹ '{readable_model_name}' (Prompt: {prompt_type}) åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆé¢„æµ‹ ---\n", "blue")
        
        try:
            model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True, torch_dtype=torch.bfloat16)
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "right"
            prompt_function = PROMPT_DICT[prompt_type]

            for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=f"æ¨ç† {readable_model_name}-{prompt_type}", leave=False):
                prompt = prompt_function(row['text'])
                
                inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512).to("cuda")
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs, max_new_tokens=10, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id
                    )
                
                output_ids = outputs[0][inputs.input_ids.shape[1]:]
                generated_text = tokenizer.decode(output_ids, skip_special_tokens=True)
                
                all_predictions.append({
                    'id': row['ID'], 'model_id': readable_model_name, 'prompt_type': prompt_type,
                    'true_label': row['corrected_status'], 'predicted_label': clean_prediction(generated_text)
                })
        except Exception as e:
            cprint(f"å¤„ç† {exp_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}", "red")
        finally:
            # æ¸…ç†å†…å­˜
            if 'model' in locals(): del model
            if 'tokenizer' in locals(): del tokenizer
            gc.collect()
            torch.cuda.empty_cache()

    predictions_df = pd.DataFrame(all_predictions)
    if not predictions_df.empty:
        output_path = os.path.join(config.OUTPUT_DIR_BASE, config.PREDICTIONS_FILENAME)
        predictions_df.to_csv(output_path, index=False)
        cprint(f"\næ‰€æœ‰é¢„æµ‹ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {output_path}", "green", attrs=['bold'])
    
    return predictions_df


def analyze_predictions(predictions_df, base_dir):
    """å¯¹é¢„æµ‹ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æï¼ŒåŒ…æ‹¬Kappaå’Œåˆ†ç±»æŠ¥å‘Šï¼Œå¹¶ç»˜å›¾ã€‚"""
    if 'predictions_df' in locals() and not predictions_df.empty:
        # --- Cohen's Kappa ---
        cprint("--> æ­£åœ¨è®¡ç®—å¹¶å¯è§†åŒ– Cohen's Kappa ç³»æ•°...", "blue")
        kappa_results = []
        for name, group in predictions_df.groupby(['model_id', 'prompt_type']):
            kappa = cohen_kappa_score(group['true_label'], group['predicted_label'])
            kappa_results.append({'model_id': name[0], 'prompt_type': name[1], 'cohen_kappa': kappa})
        
        kappa_df = pd.DataFrame(kappa_results)
        cprint("--- Cohen's Kappa ç³»æ•°è®¡ç®—ç»“æœ (åŸºäºå®Œæ•´æµ‹è¯•é›†) ---", "cyan", attrs=['bold'])
        print(kappa_df.sort_values(by='cohen_kappa', ascending=False))

        plt.figure(figsize=(12, 7))
        sns.barplot(data=kappa_df, x='model_id', y='cohen_kappa', hue='prompt_type', palette='viridis')
        plt.title("å„å®éªŒç»„åˆçš„ Cohen's Kappa ç³»æ•° (æµ‹è¯•é›†)", fontsize=16)
        plt.xlabel("æ¨¡å‹", fontsize=12); plt.ylabel("Cohen's Kappa", fontsize=12)
        plt.xticks(rotation=15, ha='right'); plt.tight_layout()
        filepath = os.path.join(base_dir, "cohen_kappa_comparison.png")
        plt.savefig(filepath, dpi=300); cprint(f"å·²ä¿å­˜å›¾è¡¨åˆ°: {filepath}", "green"); plt.show()

        # --- åˆ†ç±»æŠ¥å‘Š ---
        cprint("\n--> æ­£åœ¨è®¡ç®—å¹¶å¯è§†åŒ–è¯¦ç»†åˆ†ç±»æŒ‡æ ‡...", "blue")
        metrics = []
        for name, group in predictions_df.groupby(['model_id', 'prompt_type']):
            report = classification_report(group['true_label'], group['predicted_label'], output_dict=True, zero_division=0)
            avg = report['weighted avg']
            metrics.append({
                'model_id': name[0], 'prompt_type': name[1], 'precision': avg['precision'],
                'recall': avg['recall'], 'f1-score': avg['f1-score']
            })
        
        metrics_df = pd.DataFrame(metrics)
        cprint("--- å„å®éªŒç»„åˆçš„åˆ†ç±»æ€§èƒ½æŒ‡æ ‡ (åŸºäºå®Œæ•´æµ‹è¯•é›†, åŠ æƒå¹³å‡) ---", "cyan", attrs=['bold'])
        print(metrics_df.sort_values(by='f1-score', ascending=False))
        
        metrics_melted_df = metrics_df.melt(id_vars=['model_id', 'prompt_type'], value_vars=['precision', 'recall', 'f1-score'], var_name='metric', value_name='score')
        g = sns.catplot(
            data=metrics_melted_df, kind='bar', x='model_id', y='score', hue='prompt_type',
            col='metric', palette='magma', height=6, aspect=1.2
        )
        g.fig.suptitle('å„æ¨¡å‹ä¸Promptç»„åˆçš„æœ€ç»ˆåˆ†ç±»æ€§èƒ½æŒ‡æ ‡å¯¹æ¯” (æµ‹è¯•é›†)', fontsize=20, y=1.03)
        g.set_axis_labels("æ¨¡å‹", "åˆ†æ•°"); g.set_titles("æŒ‡æ ‡: {col_name}")
        g.set_xticklabels(rotation=15, ha='right'); g.set(ylim=(0, 1))
        g.legend.set_title("Prompt ç±»å‹")
        for ax in g.axes.flat:
            for p in ax.patches:
                ax.annotate(f'{p.get_height():.3f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                            ha='center', va='center', xytext=(0, 9), textcoords='offset points', fontsize=9)
        plt.tight_layout(rect=[0, 0, 1, 0.97])
        filepath = os.path.join(base_dir, "classification_metrics_comparison.png")
        plt.savefig(filepath, dpi=300); cprint(f"å·²ä¿å­˜å›¾è¡¨åˆ°: {filepath}", "green"); plt.show()
    else:
        cprint("é¢„æµ‹æ•°æ®æ¡†æœªæ‰¾åˆ°æˆ–ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚", "yellow")


def start_evaluation():
    """
    å¯åŠ¨å®Œæ•´çš„è¯„ä¼°æµç¨‹ï¼šæ±‡æ€»ç»“æœã€ç»˜å›¾ã€ç”Ÿæˆé¢„æµ‹ã€æ·±å…¥åˆ†æã€‚
    """
    config = Config()
    cprint("--- å¼€å§‹è¯„ä¼°æµç¨‹ ---", "cyan", attrs=['bold'])

    # 1. æ±‡æ€»è®­ç»ƒå†å²å¹¶ç»˜å›¾
    cprint("\n--- æ­¥éª¤ 1: æ±‡æ€»è®­ç»ƒå†å²å¹¶ç»˜åˆ¶æ›²çº¿å›¾ ---", "blue")
    results_df = aggregate_results(config.OUTPUT_DIR_BASE, config.PROMPTS_TO_RUN)
    if not results_df.empty:
        cprint("\næˆåŠŸæ±‡æ€»æ‰€æœ‰å®éªŒç»“æœï¼", "green", attrs=['bold'])
        print("æ±‡æ€»æ•°æ®é¢„è§ˆ (æŒ‡æ ‡åŸºäºæµ‹è¯•é›†):"); print(results_df.head())
        plot_metric_curves(results_df, config.OUTPUT_DIR_BASE)
        plot_final_performance(results_df, config.OUTPUT_DIR_BASE)
    else:
        cprint("æœªèƒ½åŠ è½½ä»»ä½•è®­ç»ƒå†å²æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾ã€‚", "yellow")

    # 2. ä¸ºæ‰€æœ‰æœ€ç»ˆæ¨¡å‹ç”Ÿæˆç»Ÿä¸€çš„é¢„æµ‹æ–‡ä»¶
    cprint("\n--- æ­¥éª¤ 2: ä¸ºæ‰€æœ‰æœ€ç»ˆæ¨¡å‹ç”Ÿæˆç»Ÿä¸€çš„é¢„æµ‹æ–‡ä»¶ ---", "blue")
    predictions_path = os.path.join(config.OUTPUT_DIR_BASE, config.PREDICTIONS_FILENAME)
    if os.path.exists(predictions_path):
        cprint(f"æ£€æµ‹åˆ°å·²å­˜åœ¨çš„é¢„æµ‹æ–‡ä»¶: {predictions_path}ã€‚å°†ç›´æ¥åŠ è½½ç”¨äºåˆ†æã€‚", "yellow")
        predictions_df = pd.read_csv(predictions_path)
    else:
        predictions_df = generate_predictions_for_all(config)
    
    # 3. å¯¹é¢„æµ‹ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æ
    if not predictions_df.empty:
        cprint("\n--- æ­¥éª¤ 3: å¯¹æœ€ç»ˆé¢„æµ‹ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æ ---", "blue")
        analyze_predictions(predictions_df, config.OUTPUT_DIR_BASE)
    else:
        cprint("æœªèƒ½ç”Ÿæˆæˆ–åŠ è½½é¢„æµ‹æ•°æ®ï¼Œæ— æ³•è¿›è¡Œè¯¦ç»†åˆ†æã€‚", "red")

    cprint("\nğŸ‰ğŸ‰ğŸ‰ è¯„ä¼°æµç¨‹å·²å®Œæˆï¼ ğŸ‰ğŸ‰ğŸ‰", "green", attrs=['bold'])